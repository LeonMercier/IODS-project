## Notes from DataCamp exercises

complete.cases() returns true if case has no NAs

correlation matrix cor()
correlation plot: cor(df) %>% corplot()

Video: dimensionality reduction techniques
dimensionality ~~ number of variables per observation

PCA
components are uncorrelated to eachother
1st captures most variance, 2nd most of remaining variance etc. -> *Order of importance*
assumes that features(variables) with mre variance are more important -> standardization of data

"There are two functions in the default package distribution of R that can be used to perform PCA: princomp() and prcomp(). The prcomp() function uses the SVD and is the preferred, more numerically accurate method."

standardization of data: scale()

prcomp(df) %>% biplot()

-> scatter plot of observations according to PC1 and PC2
arrows represet the original features 
  -> pointing same way = pos. correlation
    -> 90 deg angle = no correlation
  -> pointing same way as axis: correlated to the PC
  
Multiple correspondence analysis (MCA): like PCA but for categorical variables


## Then onto the actual course exercise

First we load the required packages. Tidyverse includes dplyr and others.
```{r echo = TRUE, message = FALSE }
library(tidyverse)
library(GGally)
```
Then we read the data from a file, make a visualisation and a summary of the variables. 
```{r}
human <- read_csv("data/human.csv")
ggpairs(human)
summary(human)
```

We use prcomp() to make the principal component analysis and biplot() to visualise the result. Then we show the amount of variability captured by each principal component, converted into percentage display. 
```{r}
pca <- prcomp(human)
round(100 * summary(pca)$importance[2,], digits = 2)
biplot(pca)
```
And we do the same as above but with properly scaled (standardized data). 
```{r}
humanStd <- scale(human)
pca <- prcomp(humanStd)
biplot(pca)

round(100 * summary(pca)$importance[2,], digits = 2)
```
Here we see that with the non-standardized data the first principal component captures nearly all of the variability of the data. THis is due to the variances of the variables being of different sizes. 

### Multiple correspondence analysis
We load the FactoMineR library and the 'tea' dataset. Then we explore the structure and dimensions of the dataset. 
```{r}
library(FactoMineR)
data(tea)
str(tea)
dim(tea)
```

Performing the Multiple correspondence analysis. 
```{r}

keep_columns <- c("Tea", "How", "how", "sugar", "where", "lunch")

# select the 'keep_columns' to create a new dataset
tea_time <- select(tea, one_of(keep_columns))
mca <- MCA(tea_time)
plot(mca, invisible = c("ind"))
```

