# Week 2: regression and model validation

*Describe the work you have done this week and summarize your learning.*

- Describe your work and results clearly. 
- Assume the reader has an introductory course level understanding of writing and reading R code as well as statistical methods.
- Assume the reader has no previous knowledge of your data or the more advanced methods you are using.

```{r}
date()
```

## First some notes from the DataCamp exercises 
the actual analysis exercise is further down

We can use a character vector to hold variable names, but then we need to use all_of() inside of select()
```{r, eval = FALSE}
vars <- c("var1", "var2", "var3")
analysis <- select(df, all_of(vars))
```

The syntax for renaming variables is rename(df, newName = oldName) no quotes
```{r, eval = FALSE}
rename(learning2014, dEEp = deep)
```

* filter() is like subset()
* geom_smooth() is used to add regression lines to plots
* pairs() a quick graphical comparison of all possible variable pairs in a data frame
* ggpairs() is same as pairs() but more advanced
  * with the 'upper' and 'lower' parameters it can be useful to use wrap()

## A couple of notes from the MABS4IODS book

On interpreting the p-value: 

> The probability of obtaining the observed data (or data that represent a more extreme departure from the null hypothesis) if the null hypothesis is true.

and 

> p-values should not be taken too seriously; confidence intervals are often more informative

Terminology: 

response variable = intercept + (slope * explanatory var) + error

## More notes from the DataCamp exercises

### Video: Linear regression

Residuals: the difference between values predicted by the model and the actual values of the response variable _y_. The model is found by minimizing the sum of the squared residuals (distances of the data points from the regression line)

Meaning of the output of summary() 
* coefficients: estimates of the model parameters  
* intercept estimate: alpha of model
* variable estimate: beta or slope of model
* t-value and p-value testing the null hypothesis that the slope is 0 (no relationship between variables)
* low p-value: we can conclude that there *IS* a relationship between the variables
  
Regression model object is created with lm() and it can also be multivariable (x1, x2, x3 ...)  
Conveniently passing the object to plot() gives diagnostic plots
```{r, eval = FALSE}
my_lm <- lm(y ~ x1 + x2 + x3 ..., data = df)
plot(my_lm)
```
  
### Video: Model validation
The assumption of the model must be confirmed to be true before it should be trusted. Considering the residuals / errors, this means that (number 3 and 4 are essentially the same thing?):
1) errors are normally distributed
2) errors are not correlated
3) errors have constant variance
4) error size does not depend on explanatory variable

So how to examine this?  
* Normal distribution of residuals: the QQ-plot
  * Residuals are normally distributed if they fall on the line of the plot
* Constant variance of errors = error size does not depend on explanatory variable
  * Plot residuals vs model predictions/fitted values
  * There should not be any kind of pattern
* Leverage means how much impact a single observation has on the model
  * Plot residuals vs leverage and you will see which observations have an unusually high impact

The lm-object we created can be used to predict using and existing model: giving new values for the explanatory variable, we can predict the dependent variable
```{r, eval = FALSE}
predict(existing_model, newdata = df)
```

## And here goes the analysis part of the 2nd week assignement

First we load the required libraries that we use in addition to the built-in R functions
```{r}
library(GGally)
library(ggplot2)
```
Then we read the already wrangled data from a file
```{r}
students2014 <- read.csv("data/learning2014.csv")
```

The data has 166 rows and 7 column, which correspond to 166 students surveyed and 7 variables measured for each. 
```{r}
dim(students2014)
```

The Attitude, deep, stra and surf variables range from 1 to 5, they were created by taking the means of several answers to a questionnaire. 
```{r}
str(students2014)
```

We can better understand the data by drawing some exploratory plots. The aesthetic aes() is used to color the plots according to gender (col) and set a transparency (alpha). The wrap() function is used to set the function and parameter used by ggpairs to draw the plots. 
```{r}
p <- ggpairs(students2014, mapping = aes(col = gender, alpha = 0.4), lower = list(combo = wrap("facethist", bins = 20)))
# draw the plot
p
```

Then comes the actual regression modeling. Attitude, stra and surf were chosen as the explanatory variables as they showed the highest correlations to Points in the exploratory plots above. 
```{r}
lm <- lm(Points ~ attitude + stra + surf, data = students2014)
```

Then we examine the model we made: 
```{r}
summary(lm)
```
We see that the coefficients for stra and surf have high p-values, in other words not statistically significant, so lets remove them (this should give the most parsimonious model)
```{r}
lm <- lm(Points ~ attitude, data = students2014)
summary(lm)
```

Then we generate some diagnostic plots to check if the model assumptions are correct. Calling plot() on a lm-object will call lm.plot(), meaning we can use the which option to select plot types. The par() function puts the plots in a nice grid. 
```{r}
par(mfrow = c(2,2))
plot(lm, which = c(1,2,5))
```




